# 模型压缩和推理加速 (Model Compression & Inference Acceleration)

## 一、概述

#### 1. 面临/解决问题

1. **速度**：实时响应效率的要求，过长的响应耗时会严重影响用户体验。
2. **存储**：有限的内存空间要求，无法加载超大模型的权重从而无法使用模型。
3. **能耗**：移动场景的续航要求，大量的浮点计算导致移动设备耗电过快。

### 2. 解决方案

针对上述三类问题，可以从**模型压缩**和**推理加速**两个角度出发，在保持一定模型精度的情况下，让模型速度更快、体积更小、能耗更低。

## 二、 模型压缩

### 0. 常用模型压缩方法

- 裁剪/剪枝：网络结构裁剪 -> 减少模型参数

- 量化 ： 将浮点运算变为整数运算 -> 减少模型运算量和体积

- 神经结构搜索： 以模型大小和推理速度为约束的神经结构搜索 -> 提高网络效率

- 知识蒸馏： 将大模型的知识迁移到小模型上 -> 提高小模型精度

#### 1. 剪裁/剪枝

## 三、推理加速

### 0. 常用推理加速方法

- 硬件加速

- 推理框架加速

- 并行计算
