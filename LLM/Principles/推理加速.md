# 模型压缩和推理加速 (Model Compression & Inference Acceleration)

## 一、概述

#### 1. 面临/解决问题

1. **速度**：实时响应效率的要求，过长的响应耗时会严重影响用户体验。
2. **存储**：有限的内存空间要求，无法加载超大模型的权重从而无法使用模型。
3. **能耗**：移动场景的续航要求，大量的浮点计算导致移动设备耗电过快。

### 2. 解决方案

针对上述三类问题，可以从**模型压缩**和**推理加速**两个角度出发，在保持一定模型精度的情况下，让模型速度更快、体积更小、能耗更低。

## 二、 模型压缩

### 0. 常用模型压缩方法

- 裁剪/剪枝：网络结构裁剪 -> 减少模型参数

- 量化 ： 将浮点运算变为整数运算 -> 减少模型运算量和体积

- 神经结构搜索： 以模型大小和推理速度为约束的神经结构搜索 -> 提高网络效率

- 知识蒸馏： 将大模型的知识迁移到小模型上 -> 提高小模型精度

#### 1. 剪裁/剪枝

## 三、推理加速

### 0. 常用推理加速方法

- 硬件加速

- 推理框架加速

- 并行计算

### 1. 减小 KV 缓存的大小是一个重要的改进方向

对于当前的大模型推理，KV 缓存占据了 GPU 内存的很大一部分，因此减小 KV 缓存的大小是一个重要的改进方向。最近，有几篇论文从不同的角度探讨了这个问题，具体对比见表，包括：

- FastDecode：此方法将 KV 缓存的所有计算卸载到 CPU。KV 缓存的计算和存储发生在 CPU 上。

- 基于量化的压缩方法（GEAR，混合精度）：通过应用各种量化技术，可以减小单个令牌 KV 缓存的大小，而不会减少 KV 缓存中存储的令牌数量。这种方法还可能导致相应的残差矩阵和异常矩阵，这些矩阵需要存储在内存中，但不能存储在 KV 缓存中。它还可能涉及量化不重要的令牌 KV 缓存，以减少 KV 缓存的内存占用。

- 部分 KV 缓存逐出（SnapKV, H2O, LESS, Adaptive Compression, Scissorhands, Dynamic Memory Compression, StreamingLLM）：通过移除一些相对无用的 KV 缓存条目，减少 KV 缓存的内存占用。从本质上讲，这减少了存储在 KV 缓存中的令牌数量，而不会减小单个令牌 KV 缓存的大小。

#### 1.3

- SnapKV:
  SnapKV是一种用于大型语言模型的高效键值(KV)缓存系统。它旨在优化LLM推理过程中的内存使用和访问速度。SnapKV通过智能管理和压缩KV缓存来减少内存占用,同时保持模型性能。
- H2O:
  H2O是一个开源的机器学习平台,提供了多种算法和工具用于数据分析、预测建模和机器学习。它支持分布式计算,可以处理大规模数据集。虽然H2O不是专门为LLM设计的,但它在AI和机器学习领域广泛应用。
- LESS (Large-scale Efficient Stable Sequence Training):
  LESS是一种训练大型语言模型的方法,旨在提高训练效率和稳定性。它通过优化训练过程中的数据处理和模型更新策略,减少计算资源需求,同时保持或提高模型性能。
- Adaptive Compression:
  自适应压缩是一种动态调整数据压缩级别的技术。在LLM中,这可以用于优化模型权重和激活值的存储,根据不同层和计算阶段的需求自动调整压缩率,以平衡内存使用和计算效率。
- Scissorhands:
  Scissorhands是一种模型裁剪技术,用于减小LLM的大小。它通过识别和移除不重要的神经元或连接来压缩模型,同时尽量保持模型的性能。这种方法可以使大型模型更适合在资源受限的环境中部署。
- Dynamic Memory Compression:
  动态内存压缩是一种在LLM推理过程中动态管理内存使用的技术。它可以根据当前任务的需求和可用资源,实时调整内存分配和压缩策略,以优化性能和资源利用。
- StreamingLLM:
  StreamingLLM是一种允许LLM处理无限长度输入的技术。传统LLM通常有固定的上下文窗口大小,而StreamingLLM通过高效管理内存和注意力机制,使模型能够连续处理长序列数据,如长文档或实时流数据。

## 四、技术列表

- **Attention Structure**
  - MHA, MQA, GQA, MLA
- **Attention Speed up**
  - FA, PA, VA, Mamba, Infini-Attn
- **Reduction Dimension**
  - LoRA, QLORA, DFT, MLA
- **Reduction Calculate**
  - SnapKV, PyramidKV, KvCache,
- (Partial KV cache evictio)
  - (SnapKV, H2O, LESS, Adaptive Compression, Scissorhands, Dynamic Memory Compression, StreamingLLM)
- **Parallel Calculate**
  - TP, DP, PP, MP, MoE (EP)
- **Parallel Structure**
  - Speculative Decoding, Medusa, MoE, MoT, MoD
- **Parallel Pipeline**
  - Cellular Batching, Continue Batching, SplitFuse, LightLLM, Infight Batch
- **Quantity**
  - AWQ, GPTQ, GGUF, MatMult-free
- **Infer System**
  - Triton, vLLM, DeepSeed, TGI
- **Data System**
  - AlStore, Alluxio, JuiceFS
- **Matrix**
  - CUDA, CANN
- **Device Side**
  - tensorRT, MNN, Tengine, TF-light, Paddle-mobile, MACE, MindSpore-light, ONNX, TNN, Mini-Caffe
- **Other**
  - Post-Process: Greedy Search, BeamSearch, Top-k, Top-p, Temperature
  - Multimodal: SPHINX, Sora, Stable Diffusion
  - LangChain: LangGraph, Agent, RALMs, MoT (Memory-of-Thought)
  - FastDecode: 此方法将 KV 缓存的所有计算卸载到 CPU。KV 缓存的计算和存储发生在 CPU 上。

### 1. Attention Structure

#### 1.0 瓶颈

一个自然的问题是：为什么降低KV Cache的大小如此重要？

众所周知，一般情况下LLM的推理都是在GPU上进行，单张GPU的显存是有限的，一部分我们要用来存放模型的参数和前向计算的激活值，这部分依赖于模型的体量，选定模型后它就是个常数；另外一部分我们要用来存放模型的KV Cache，这部分不仅依赖于模型的体量，还依赖于模型的输入长度，也就是在推理过程中是动态增长的，当Context长度足够长时，它的大小就会占主导地位，可能超出一张卡甚至一台机（8张卡）的总显存量。

在GPU上部署模型的原则是：能一张卡部署的，就不要跨多张卡；能一台机部署的，就不要跨多台机。这是因为“卡内通信带宽 > 卡间通信带宽 > 机间通信带宽”，由于“木桶效应”，模型部署时跨的设备越多，受设备间通信带宽的的“拖累”就越大，事实上即便是单卡H100内SRAM与HBM的带宽已经达到了3TB/s，但对于Short Context来说这个速度依然还是推理的瓶颈，更不用说更慢的卡间、机间通信了。

所以，减少KV Cache的目的就是要实现在更少的设备上推理更长的Context，或者在相同的Context长度下让推理的batch size更大，从而实现更快的推理速度或者更大的吞吐总量。当然，最终目的都是为了实现更低的推理成本。

要想更详细地了解这个问题，读者可以进一步阅读《FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness》、《A guide to LLM inference and performance》、《LLM inference speed of light》等文章，这里就不继续展开了（主要是笔者水平也有限，唯恐说多错多）。

#### 1.1 MHA(Multi-Head Attention)

MHA（Multi-Head Attention），也就是多头注意力，是开山之作《Attention is all you need》所提出的一种Attention形式，可以说它是当前主流LLM的基础工作。

而后面的MQA、GQA、MLA，都是围绕“如何减少KV Cache同时尽可能地保证效果”这个主题发展而来的产物。

#### 1.2 MQA(Multi-Query Attention)

MQA(Multi-Query Attention)是减少KV Cache的一次非常朴素的尝试，首次提出自《Fast Transformer Decoding: One Write-Head is All You Need》，这已经是2019年的论文了，这也意味着早在LLM火热之前，减少KV Cache就已经是研究人员非常关注的一个课题了。

MQA的思路很简单，直接让所有Attention Head共享同一个K、V。

使用MQA的模型包括PaLM、StarCoder、Gemini等。很明显，MQA直接将KV Cache减少到了原来的1/h，这是非常可观的，单从节省显存角度看已经是天花板了。

效果方面，目前看来大部分任务的损失都比较有限，且MQA的支持者相信这部分损失可以通过进一步训练来弥补回。此外，注意到MQA由于共享了K、V，将会导致Attention的参数量减少了将近一半，而为了模型总参数量的不变，通常会相应地增大FFN/GLU的规模，这也能弥补一部分效果损失。

#### 1.2 GQA（Grouped-Query Attention）

然而，也有人担心MQA对KV Cache的压缩太严重，以至于会影响模型的学习效率以及最终效果。为此，一个MHA与MQA之间的过渡版本GQA（Grouped-Query Attention）应运而生，出自论文《GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints》，是去年的工作。

事后看来，GQA的思想也很朴素，它就是将所有Head分为g 个组（g 可以整除h），每组共享同一对K、V。

GQA最知名的使用者，大概是Meta开源的LLAMA2-70B，以及LLAMA3全系列，此外使用GQA的模型还有TigerBot、DeepSeek-V1、StarCoder2、Yi、ChatGLM2、ChatGLM3等，相比使用MQA的模型更多（ChatGLM虽然在它的介绍中说自己是MQA，但实际是g=2
的GQA）。

在llama2/3-70B中，GQA的g=8，其他用了GQA的同体量模型基本上也保持了这个设置，这并非偶然，而是同样出于推理效率的考虑。我们知道，70B这个体量的模型，如果不进行极端的量化，那么不可能部署到单卡（A100/H100 80G）上。单卡不行，那么就能单机了，一般情况下一台机可以装8张卡，刚才我们说了，Attention的每个Head实际上是独立运算然后拼接起来的，当g=8
时，正好可以每张卡负责计算一组K、V对应的Attention Head，这样可以在尽可能保证K、V多样性的同时最大程度上减少卡间通信。

#### 1.4 MLA（Multi-head Latent Attention）

有了MHA、MQA、GQA的铺垫，我们理解MLA（**M**ulti-head **L**atent **A**ttention）就相对容易一些了。DeepSeek-V2的技术报告里是从低秩投影的角度引入MLA的，以至于有部分读者提出“为什么LoRA提出这么久了，直到MLA才提出对KV Cache低秩分解的做法”之类的疑问。

然而，笔者认为低秩投影这个角度并不贴近本质，因为要说低秩投影的话，事实上只要我们将GQA的所有K、V叠在一起，就会发现GQA也相当于在做低秩投影

### 2. Attention Speed up

#### 2.1 FA(Flash Attention)

Flash Attention 一种优化注意力计算的方法,通过重组计算顺序和利用GPU内存层次结构来提高效率。

#### 2.2 PA(Paged Attention)

Paged Attention 一种内存管理技术,允许在GPU和CPU内存之间分页存储注意力状态,以处理超长序列。

#### 2.3 VA(Vitual Attention)

可能指的是Virtual Attention,一种处理长序列的注意力机制,通过虚拟化注意力来扩展序列长度。

#### 2.4 Mamba

Mamba 一种基于状态空间模型(SSM)的架构,作为Transformer的替代方案,特别适合处理长序列数据。

#### 2.5 Infini-Attn

Infini-Attn: Infinite Attention 一种处理无限长度序列的注意力机制,通过递归计算来实现对任意长度序列的处理。

### 3. Reduction Dimension

#### 3.1 LoRA(Low-Rank Adaptation)

LoRA是一种用于微调大型语言模型的高效方法。它通过添加少量可训练参数来适应新任务,而不需要更新模型的所有权重。这种方法大大减少了计算需求和存储空间。

#### 3.2 QLORA(Quantized Low-Rank Adaptation)

QLoRA是LoRA的进一步优化版本。它结合了量化技术和低秩适应,允许在更低的内存消耗下微调大型语言模型。这使得在消费级硬件上微调大型模型成为可能。

#### 3.3 DFT(Dynamic Factorization Training)

DFT是一种用于降低矩阵秩的技术，主要应用于大型语言模型的训练过程中。它的目标是通过动态分解来减少模型的参数数量，从而降低计算复杂度和内存需求。这种方法可以帮助在有限的计算资源下训练更大的模型，或者加速现有模型的训练过程。

#### 3.4 MLA（**M**ulti-head **L**atent **A**ttention）

详见 1.4

### 4. Reduction Calculate

#### 4.1 SnapKV

我们发现模型中的每个注意力头在生成过程中始终关注特定的即时注意力特征。 同时，可以从位于提示末尾的“观察”窗口获得这种稳健的模式。 利用这一见解，**SnapKV 通过为每个注意力头选择聚集的重要 KV 位置来自动压缩 KV 缓存**。 我们的方法显着减少了处理长输入序列时不断增长的计算开销和内存占用。 具体来说，在处理 16K Token 的输入时，与基线相比，SnapKV 实现了一致的解码速度，生成速度提高了 3.6 倍，内存效率提高了 8.2 倍。 同时，它在 16 个长序列数据集上保持了与基线模型相当的性能。 此外，SnapKV 可以使用 HuggingFace 实现在单个 A100-80GB GPU 上处理多达 380K 上下文 Token ，只需进行微小的更改，在大海捞针测试中仅表现出可以忽略不计的准确性下降。 进一步的综合研究表明SnapKV具有实际应用的潜力。

SnapKV 是一种有效而简单的解决方案，可压缩 KV 缓存以减轻处理大量提示的计算和内存负担。 观察到提示中的特定标记在生成过程中获得了每个头的一致关注，我们的方法不仅检索了关键信息，而且还提高了处理效率。 尽管有其优势，SnapKV 的范围主要局限于模型的生成方面，特别是针对生成过程中的 KV 缓存。 此限制意味着如果模型本身就难以处理长上下文或表现不佳，则 SnapKV 无法扩展模型的长上下文功能。 此外，SnapKV的设计并未涵盖提示推理的处理，这限制了其在系统无法处理超长提示的场景中的有效性。 尽管如此，我们的贡献为社区提供了重要的见解和工具，为管理大规模语言建模挑战的更精细的方法铺平了道路。 附录提供了更多并行解码实验以及有关生成加速的讨论。

#### 4.2 PyramidKV

在本研究中，我们研究了大型语言模型（大语言模型）内基于注意力的信息流是否通过可察觉的模式进行聚合，以进行长上下文处理。 我们的观察表明，大语言模型通过金字塔信息漏斗聚合信息，其中注意力广泛分散在较低层中，在特定上下文中逐步巩固，并最终集中在关键 Token （又称为大规模激活或注意力沉降）上。更高层。 受这些见解的启发，我们开发了 PyramidKV，**一种新颖且有效的 KV 缓存压缩方法**。 这种方法动态调整不同层的 KV 缓存大小，在较低层分配更多缓存，在较高层分配较少缓存，这与保持统一 KV 缓存大小的传统方法不同。 我们利用 LongBench 基准进行的实验评估表明，PyramidKV 与具有完整 KV 缓存的模型的性能相匹配，同时仅保留 12% 的 KV 缓存，从而显着减少了内存使用量。 在强调内存效率、仅维护 0.7% 的 KV 缓存的场景中，PyramidKV 超越了其他 KV 缓存压缩技术，在 TREC 上实现了高达 20.5 的绝对精度提升。

在本研究中，我们研究了大语言模型（大语言模型）在处理长上下文输入时的内在注意力模式。 我们的实证分析使我们发现跨层注意力中存在金字塔信息漏斗：它将信息广泛分布在较低层中，逐渐将其集中在特定上下文中，并最终集中在具有大量激活或注意力沉降的较高层中的关键标记上。 受这一发现的启发，我们设计了一种利用这种信息流模式的新型 KV 缓存压缩方法 PyramidKV。 它跨层改变 KV 缓存大小，其独特设计是为了补充跨不同层观察到的注意力行为。 通过将金字塔信息漏斗运用到 KV 缓存压缩设计中，我们的方法在内存受限的设置中表现出色，保留了长上下文理解能力，并且与基线相比，以最小的性能权衡显着减少了内存使用量。

#### 4.3 KvCache

## 参考资料

- Attention Structure

https://spaces.ac.cn/archives/10091

- vLLM  will  Support sparse KV cache framework？

https://github.com/vllm-project/vllm/issues/5751

- manba cache control

https://github.com/vllm-project/vllm/pull/4115

- MQA、GQA及 FlashAttention 相关

https://github.com/vllm-project/vllm/pull/2401

- AI论文-中英对照

https://yiyibooks.cn/

- 详细

https://www.cnblogs.com/88223100/p/Technical_principles_and_details_of_mainstream_large_language_models.html
