# Treansformers快速开始

## 代码

- 导入所需的库,包括PyTorch和Transformers库中的自动模型和分词器。

```python
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
```

- 创建分词器,从指定路径加载预训练的分词器,并设置一些参数。

```python
print("Creating tokenizer...")
tokenizer = AutoTokenizer.from_pretrained('/TRTDir/Models/Yuan2.0-2B-hf', add_eos_token=False, add_bos_token=False, eos_token='<eod>')
```

- 创建模型,从同一路径加载预训练的因果语言模型,设置设备映射,使用float16精度,并信任远程代码。

```python
print("Creating model...")
# YuanForCausalLM
model = AutoModelForCausalLM.from_pretrained('/TRTDir/Models/Yuan2.0-2B-hf', device_map='auto', torch_dtype=torch.float16, trust_remote_code=True)
```

- 设置设备,如果可用则使用GPU,否则使用CPU。

```python
# 设置设备
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
```

- 准备输入文本,并将其编码为模型可以理解的格式,然后移至指定设备。

```python
# 准备输入
input_text = "请问目前最先进的机器学习算法有哪些？"
input_ids = tokenizer.encode(input_text, return_tensors="pt").to(device)
```

- 设置生成参数,包括最大新生成的token数、温度、top-p采样、重复惩罚、流式输出间隔和停止token。

```python
# 设置生成参数
max_new_tokens = 512
temperature = 0.7
top_p = 0.95
repetition_penalty = 1.0
stream_interval = 2
stop_token_ids = [tokenizer.eos_token_id]
```

- 初始化输出ID列表,并记录输入长度。

```python
output_ids = input_ids.tolist()[0]  # list 
input_echo_len = len(output_ids)
```

- 开始迭代

```bash
for i in range(max_new_tokens):
    # 使用新的调用方式
    # CausalLMOutputWithPast
    out = model(torch.as_tensor([output_ids], device=device), use_cache=True)

    logits = out.logits  # 预测值 3D Tensor
    past_key_values = out.past_key_values # ? 4D tensor

    last_token_logits = logits[0, -1, :]  # 1D Tensor

    if temperature < 1e-5 or top_p < 1e-8:  # greedy
        _, indices = torch.topk(last_token_logits, 2)
        tokens = [int(index) for index in indices.tolist()]
    else:
        probs = torch.softmax(last_token_logits / temperature, dim=-1)
        indices = torch.multinomial(probs, num_samples=2) # 1D Tensor (2,)
        tokens = [int(token) for token in indices.tolist()] # List

    token = tokens[0]
    output_ids.append(token)  # List

    if token in stop_token_ids:
        stopped = True
    else:
        stopped = False

    if i % stream_interval == 0 or i == max_new_tokens - 1 or stopped:
        tmp_output_ids = output_ids[input_echo_len:] # 截取问题后的内容
        output = tokenizer.decode(
            tmp_output_ids,
            skip_special_tokens=True,
            spaces_between_special_tokens=False,
            clean_up_tokenization_spaces=True,
        )
        print(output, end="\n", flush=True)

    if stopped:
        break

print("\n生成完成")
```

### 1

```python
last_token_logits = logits[0, -1, :]
```

根据你提供的 `logits` 的形状信息 `(batch_size, sequence_length, config.vocab_size)`，我们可以逐个解释这个索引操作：

1. `logits[0]`:
   - 这选择了 batch 中的第一个样本。因为 batch_size 是第一个维度，所以 `[0]` 表示选择第一个（也可能是唯一的一个）样本。
2. `logits[0, -1]`:
   - `-1` 在 Python 中表示序列的最后一个元素。在这里，它选择了序列中的最后一个位置。
   - 这意味着我们选择了输入序列中最后一个 token 的预测结果。
3. `logits[0, -1, :]`:
   - `:` 表示选择所有元素。在这个上下文中，它选择了词汇表中所有可能的下一个 token 的分数。

综合起来，`last_token_logits = logits[0, -1, :]` 的含义是：

- 从模型输出中选择第一个（或唯一的）样本。
- 选择该样本中最后一个位置的预测。
- 获取该位置对应词汇表中所有可能 token 的预测分数。

`last_token_logits` 的形状将是 `(config.vocab_size,)`，包含了**词汇表中每个 token 作为下一个 token 的未归一化概率**（logits）。这些 logits 随后可以用于采样或选择下一个要生成的 token。

##### 问题

为什么包括输入token的概率：

- 在训练过程中，模型学习预测每个token给定其前面所有token的条件概率。
- 这包括输入序列中的token，因为模型需要学会在给定前文的情况下正确预测每个token。

概率不是1的原因：

- logits是未归一化的分数，不直接等同于概率。
- 即使对于已知的输入token，模型也可能不会给出100%的确定性（logits转化为概率后也不会是1）。
- 这反映了模型的不确定性，也是语言模型泛化能力的一部分。

#### 为什么 input_id 可以只是单个 token

```python
out = model(
    input_ids=torch.as_tensor(
        [[token] if not sent_interrupt else output_ids], device=device
    ),
    use_cache=True,
    past_key_values=past_key_values if not sent_interrupt else None,
)
```

- KV缓存（Key-Value Cache）机制：
  - 这段代码利用了KV缓存机制，这是一种用于加速自回归语言模型推理的技术。
  - 当`use_cache=True`时，模型会保存之前计算的key和value，存储在`past_key_values`中。
- 正常生成模式（sent_interrupt 为 False）：
  - 当`sent_interrupt`为False时，只传入最新的token `[[token]]`。
  - 使用`past_key_values`中存储的之前的计算结果。
  - 这种方式大大减少了计算量，因为不需要重新处理之前的所有tokens。
- 中断后重新开始（sent_interrupt 为 True）：
  - 当`sent_interrupt`为True时，传入所有的`output_ids`。
  - `past_key_values`设为None，表示不使用之前的缓存。
  - 这种情况下，模型会重新处理整个序列。
- 两种方式的区别：
  - 只传最新token：更快，利用了之前的计算结果。
  - 传所有tokens：更慢，但允许在中断后重新开始，确保上下文的完整性。
- 为什么可以只传最新token：
  - 自注意力机制允许模型访问之前所有的tokens的信息。
  - 通过KV缓存，之前tokens的信息已经被编码并存储。
  - 只需要处理新的token，然后将其信息整合到现有的上下文中。
- 性能影响：
  - 只传最新token大大提高了生成速度，特别是在生成长文本时。
  - 对于每个新token，计算复杂度从O(n)减少到O(1)，其中n是序列长度。
- 使用场景：
  - 正常生成：使用缓存，只传新token。
  - 中断后重新开始：不使用缓存，传入全部tokens，确保上下文的一致性。

## 一、内容

### 1. CausalLMOutputWithPast

CausalLMOutputWithPast 是 Hugging Face Transformers 库中用于因果语言模型(Causal Language Models)的输出结构体。这个结构体通常用于像 GPT 系列这样的自回归语言模型。让我们来看看它的主要组成部分：

1. loss (可选, torch.FloatTensor of shape (1,))：
   - 如果在模型输入中提供了标签,这个字段会包含语言模型的损失值。
   - 用于模型训练时评估模型性能。
2. logits (torch.FloatTensor of shape (batch_size, sequence_length, config.vocab_size))：
   - 这是模型对每个输入 token 预测下一个 token 的原始输出分数(未归一化的概率)。
   - logits 的形状反映了输入批次大小、序列长度和词汇表大小。
3. past_key_values (tuple(tuple(torch.FloatTensor)))：
   - 包含预计算的键和值向量,用于加速序列生成。
   - 这是一个元组的元组,外层元组的长度等于模型层数,内层元组包含键和值。
   - past_key_values 就是实现 **KV Cache** 的方式。
4. hidden_states (可选, tuple(torch.FloatTensor))：
   - 如果输出隐藏状态的选项被激活,这个字段会包含所有层的隐藏状态。
   - 每个元素的形状为 (batch_size, sequence_length, hidden_size)。
5. attentions (可选, tuple(torch.FloatTensor))：
   - 如果输出注意力权重的选项被激活,这个字段会包含所有层的注意力权重。
   - 可用于分析模型的注意力模式。
6. cross_attentions (可选, tuple(torch.FloatTensor))：
   - 仅在使用交叉注意力的模型(如编码器-解码器模型)中可用。
   - 包含交叉注意力层的注意力权重。

##### 1111

- KV Cache 的概念：
  KV Cache 是一种用于提高自回归语言模型（如 GPT 系列）推理速度的技术。它存储了之前计算过的注意力层的 key 和 value 张量，以便在生成序列的后续步骤中重复使用。
- past_key_values 作为 KV Cache：
  - 在 Transformers 库中，past_key_values 就是实现 KV Cache 的方式。
  - 它是一个嵌套元组，包含了模型所有层的 key 和 value 张量。
- 工作原理：
  - 在生成序列的第一步，past_key_values 为空。
  - 模型计算完当前 token 的 key 和 value 后，将它们存储在 past_key_values 中。
  - 在下一步生成时，模型会使用 past_key_values 中存储的信息，只需要计算新 token 的 key 和 value，然后将它们添加到现有的 cache 中。
- 优势：
  - 显著减少计算量：不需要重新计算之前的 tokens 的 key 和 value。
  - 加速推理：特别是在生成长序列时，能够大大提高生成速度。
- 在代码中的应用：
  - 当你看到 `use_cache=True` 这样的参数时，它通常指的就是启用 KV Cache。
  - 在循环生成过程中，你会看到 past_key_values 被传递和更新。
- 内存考虑：
  - 使用 KV Cache 会增加内存使用，因为需要存储所有之前的 key 和 value。
  - 在处理非常长的序列时，可能需要考虑内存限制。

## 参考资料
