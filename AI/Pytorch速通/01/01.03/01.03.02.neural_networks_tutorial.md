## Neural Networks-损失函数-总结

- **(1) 损失函数接受一对 (output, target) 作为输入，计算网络输出和目标值差距。**
- **(2) `nn.MSELoss`函数计算输出和目标间的均方误差。**
  - `criterion = nn.MSELoss()`
  - `loss = criterion(output, target)`

## Neural Networks-损失函数

一个损失函数接受一对 (output, target) 作为输入，计算一个值来估计网络的输出和目标值相差多少。

***译者注：output为网络的输出，target为实际值***

nn包中有很多不同的[损失函数](https://pytorch.org/docs/nn.html#loss-functions)。
``nn.MSELoss``是一个比较简单的损失函数，它计算输出和目标间的**均方误差**，
例如：

```python
output = net(input)
target = torch.randn(10)  # 随机值作为样例
target = target.view(1, -1)  # 使target和output的shape相同
criterion = nn.MSELoss()

loss = criterion(output, target)
print(loss)
```

    tensor(0.8109, grad_fn=<MseLossBackward>)

现在，如果在反向过程中跟随``loss`` ， 使用它的
``.grad_fn`` 属性，将看到如下所示的计算图。

::

    input -> conv2d -> relu -> maxpool2d -> conv2d -> relu -> maxpool2d
          -> view -> linear -> relu -> linear -> relu -> linear
          -> MSELoss
          -> loss

所以，当我们调用 ``loss.backward()``时,整张计算图都会
根据loss进行微分，而且图中所有设置为``requires_grad=True``的张量
将会拥有一个随着梯度累积的``.grad`` 张量。

为了说明，让我们向后退几步:

```python
print(loss.grad_fn)  # MSELoss
print(loss.grad_fn.next_functions[0][0])  # Linear
print(loss.grad_fn.next_functions[0][0].next_functions[0][0])  # ReLU
```

    <MseLossBackward object at 0x7f3b49fe2470>
    <AddmmBackward object at 0x7f3bb05f17f0>
    <AccumulateGrad object at 0x7f3b4a3c34e0>
