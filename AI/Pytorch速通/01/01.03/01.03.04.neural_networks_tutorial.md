## Neural Networks-更新权重-总结

- (1) **`torch.optim`实现了SGD、Adam、RMSPROP等权重更新规则。**

```python
import torch.optim as optim
optimizer = optim.SGD(net.parameters(), lr=0.01)
optimizer.zero_grad()   # 权重清零
output = net(input)
loss = criterion(output, target)
loss.backward()
optimizer.step()        # 更新权重
```

## Neural Networks-更新权重

在实践中最简单的权重更新规则是随机梯度下降（SGD）：

     ``weight = weight - learning_rate * gradient``

我们可以使用简单的Python代码实现这个规则：

```python
learning_rate = 0.01
for f in net.parameters():
    f.data.sub_(f.grad.data * learning_rate)
```

但是当使用神经网络是想要使用各种不同的更新规则时，比如SGD、Nesterov-SGD、Adam、RMSPROP等，PyTorch中构建了一个包``torch.optim``实现了所有的这些规则。
使用它们非常简单：

```python
import torch.optim as optim

# create your optimizer
optimizer = optim.SGD(net.parameters(), lr=0.01)

# in your training loop:
optimizer.zero_grad()   # zero the gradient buffers
output = net(input)
loss = criterion(output, target)
loss.backward()
optimizer.step()    # Does the update
```

.. 注意::

      观察如何使用``optimizer.zero_grad()``手动将梯度缓冲区设置为零。
      这是因为梯度是按Backprop部分中的说明累积的。

```python

```
