### 1.1 Scrapy概念

是用 Python 实现的一个为了爬取网站数据、提取结构性数据而编写的应用框架。

### 1.2 组成

- **Scrapy Engine（引擎）**: 负责Spider、ItemPipeline、Downloader、Scheduler中间的通讯，信号、数据传递等。
- **Scheduler（调度器）**: 它负责接受引擎发送过来的Request请求，并按照一定的方式进行整理排列，入队，当引擎需要时，交还给引擎。
- **Downloader（下载器）**：负责下载Scrapy Engine（引擎）发送的所有Requests请求，并将其获取到的Responses交还给Scrapy Engine（引擎），由引擎交给Spider来处理。
- **Spider（爬虫）**：它负责处理所有Responses,从中分析提取数据，获取Item字段需要的数据，并将需要跟进的URL提交给引擎，再次进入Scheduler（调度器）。
- **Item Pipeline（管道）**：它负责处理Spider中获取到的Item，并进行进行后期处理（详细分析、过滤、存储等）的地方。
- **Downloader Middlewares（下载中间件）**：你可以当作是一个可以自定义扩展下载功能的组件。
- **Spider Middlewares（Spider中间件）**：你可以理解为是一个可以自定扩展和操作引擎和Spider中间通信的功能组件（比如进入Spider的Responses和从Spider出去的Requests）。

### 1.4 安装及项目结构

##### 1.4.1 安装及创建项目

```bash
# (0) 安装
pip install scrapy

# (1) 创建scrapy项目
scrapy startproject demo        # 在当前目录下创建一个名为demo的scrapy项目
# 目录结构如下：
# cmdDir/                    # 项目路径
#     scrapy.cfg             # 项目的主配置信息（真正配置在settings.py文件中）
#     demo/                  # 项目的Python模块，将会从这里引用代码
#         __init__.py
#         items.py           # 项目的目标文件，设置数据存储模板，用于结构化数据
#         pipelines.py       # 项目的管道文件，数据的持久化处理。
#         settings.py        # 项目的设置文件，如：递归的层数、并发数、延迟下载等
#         spiders/           # 储爬虫代码目录，编写爬虫解析规则的地方
#             __init__.py


# (3) 新建爬虫程序
scrapy genspider myTitle "www.baidu.com"  # myTitle爬虫名称 随后为其实url地址
# cmdDir/ 
#     demo/
#         spiders/           
#             __init__.py
#             myTitle.py     # 生成的基本爬虫程序
```

##### 1.4.2 Python爬虫文件

- 基本爬虫程序(三个强制的属性和一个方法)

```python
import scrapy
class MyTitleSpider(scrapy.Spider):           
    name = 'myTitle'                        # 爬虫名识别称，项目内唯一
    allowed_domains = ['www.baidu.com']     # 搜索域名范围
    start_urls = ['http://www.baidu.com/']  # 爬取起始URL列表，子URL从中派生

    def parse(self, response):     # 解析方法，传入起始/子URL返回的Response
                                   # (1) 解析网页数据(response.body)
                                   # (2) 提取信息生成下一个URL请求(生成item)
        context = response.xpath('/html/head/title/text()')# 提取网站标题
        title = context.extract_first()
        print(title)
        pass                       
```

- settings.py文件

```python
ROBOTSTXT_OBEY = False #忽略或者不遵守robots协议
```

##### 1.4.3 运行爬虫程序

- 通过终端直接运行

```bash
scrapy crawl myTitle                # 在项目的目录执行
scrapy crawl titleSpider --nolog    # 不输出日志
```

- 使用IDE运行

```python
# 项目根目录下（与scrapy.cfg同级）创建一个py文件，如：start.py，写入如下指令
from scrapy.cmdline import execute 
execute(['scrapy', 'crawl', 'titleSpider','--nolog']) # 爬虫起始入口
```

*
